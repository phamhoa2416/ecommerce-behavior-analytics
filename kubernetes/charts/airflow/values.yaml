airflow:
  name: airflow

  executor: LocalExecutor  # Options: LocalExecutor, CeleryExecutor, KubernetesExecutor

  image:
    repository: apache/airflow
    tag: "2.8.0"
    pullPolicy: IfNotPresent

  webserver:
    replicas: 1
    port: 8080
    resources:
      limits:
        cpu: 1000m
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 1Gi

  scheduler:
    replicas: 1
    resources:
      limits:
        cpu: 1000m
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 1Gi

  service:
    type: ClusterIP
    port: 8080

  ingress:
    enabled: false
    className: ""
    annotations: {}
    hosts:
      - host: airflow.local
        paths:
          - path: /
            pathType: ImplementationSpecific
    tls: []

  # Airflow configuration
  config:
    core:
      parallelism: 32
      dagConcurrency: 16
      maxActiveRunsPerDag: 16
      loadExamples: false
      executor: LocalExecutor
      # Connection string will be constructed in the secret template using the shared postgres service
      sqlAlchemyConn: ""  # Will be set by template
      fernetKey: "uYbZN0Qc7EseNeKZlEYfEM0c2gy7gFL9OUQ_J0rsCSc="
      defaultTimezone: "UTC"
    
    webserver:
      exposeConfig: false
      defaultUiTimezone: "UTC"
      enableProxyFix: true
    
    logging:
      loggingLevel: INFO
      fabLoggingLevel: WARN
    
    api:
      authBackend: airflow.api.auth.backend.basic_auth

  # Connections configuration
  connections:
    kafka:
      connType: kafka
      host: kafka-0.kafka-headless
      port: 9092
      extra: '{"bootstrap_servers": "kafka-0.kafka-headless:9092"}'
    
    hdfs:
      connType: hdfs
      host: hadoop-namenode
      port: 9000
      extra: '{"namenode": "hdfs://hadoop-namenode:9000"}'
    
    clickhouse:
      connType: http
      host: clickhouse
      port: 8123
      login: admin
      password: password
      schema: ecommerce

  # Variables
  variables:
    kafka_bootstrap_servers: "kafka-0.kafka-headless:9092"
    kafka_topic: "ecommerce_data"
    hdfs_namenode: "hadoop-namenode:9000"
    hdfs_output_path: "hdfs://hadoop-namenode:9000/data/ecommerce/batch"
    clickhouse_host: "clickhouse"
    clickhouse_port: "8123"
    clickhouse_database: "ecommerce"
    spark_master: "local[*]"
    spark_jars_packages: "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,com.clickhouse:clickhouse-jdbc:0.6.0"

  # DAGs configuration
  dags:
    gitSync:
      enabled: false
      repo: ""
      branch: ""
      subPath: ""
    persistence:
      enabled: true
      size: 1Gi
      storageClass: ""

  # Extra environment variables
  # Note: AIRFLOW__CORE__EXECUTOR, AIRFLOW__CORE__SQL_ALCHEMY_CONN, and AIRFLOW__CORE__FERNET_KEY
  # are already set in the deployment templates. Add only additional env vars here.
  env: []

# PostgreSQL is now managed by the shared postgres chart at the root level
# The postgres service name will be: {{ .Release.Name }}-postgres
postgres:
  serviceName: postgres
  database: airflow
  user: airflow
  password: airflow
  port: 5432

