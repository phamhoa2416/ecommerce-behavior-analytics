apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-ecommerce-job
  namespace: default
data:
  ecommerce_stream.py: |
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import from_json, col, to_timestamp, regexp_replace
    from pyspark.sql.types import StructType, StringType, DoubleType

    spark = SparkSession.builder \
        .appName("ECommerceStream") \
        .getOrCreate()

    # Schema
    schema = StructType() \
      .add("event_time", StringType()) \
      .add("event_type", StringType()) \
      .add("product_id", StringType()) \
      .add("category_id", StringType()) \
      .add("category_code", StringType()) \
      .add("brand", StringType()) \
      .add("price", DoubleType()) \
      .add("user_id", StringType()) \
      .add("user_session", StringType())

    # Kafka source
    raw_df = spark.readStream \
      .format("kafka") \
      .option("kafka.bootstrap.servers", "kafka-cluster-kafka-bootstrap.kafka.svc.cluster.local:9092") \
      .option("subscribe", "ecommerce_behavior") \
      .option("startingOffsets", "earliest") \
      .load()

    # Parse JSON
    parsed_df = raw_df \
      .select(from_json(col("value").cast("string"), schema).alias("data")) \
      .select("data.*")

    # Clean timestamp
    clean_df = parsed_df \
      .withColumn("event_time", regexp_replace("event_time", " UTC", "")) \
      .withColumn("event_time", to_timestamp("event_time", "yyyy-MM-dd HH:mm:ss")) \
      .filter(col("event_type").isNotNull())

    # Function to write each micro-batch
    def write_to_clickhouse(df, epoch_id):
        df.write \
          .format("jdbc") \
          .option("url", "jdbc:clickhouse://chi-clickhouse-clickhouse-0-0.clickhouse.svc.cluster.local:8123/ecommerce") \
          .option("driver", "com.clickhouse.jdbc.ClickHouseDriver") \
          .option("dbtable", "user_behavior_realtime") \
          .option("user", "admin") \
          .option("password", "password") \
          .mode("append") \
          .save()

    # The streaming query (IMPORTANT: OUTSIDE THE FUNCTION)
    query = clean_df.writeStream \
        .foreachBatch(write_to_clickhouse) \
        .outputMode("append") \
        .trigger(processingTime='10 seconds') \
        .start()

    query.awaitTermination()
